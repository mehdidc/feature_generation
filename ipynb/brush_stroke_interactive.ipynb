{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys, os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['THEANO_FLAGS'] = 'device=cpu,compiledir_format=\"ipynb_compiledir_%(platform)s-%(processor)s-%(python_version)s-%(python_bitwidth)s\"'\n",
    "sys.path.append(os.getcwd()+\"/..\")\n",
    "\n",
    "from tasks import check as load_filename\n",
    "from scripts.imgtovideo import imgs_to_video\n",
    "from data import load_data\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne import layers as L\n",
    "\n",
    "from lasagnekit.misc.plot_weights import dispims_color, tile_raster_images\n",
    "\n",
    "from IPython.display import HTML, Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import resize\n",
    "\n",
    "def load_model(filename, **kw):\n",
    "\n",
    "    model = load_filename(\n",
    "        what=\"notebook\", \n",
    "        filename=filename, \n",
    "        **kw\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_brush_func(layers):\n",
    "    if 'biased_output' in layers:\n",
    "        bias = layers['biased_output'].b.get_value()\n",
    "    elif 'bias' in layers:\n",
    "        bias = layers['bias'].b.get_value()\n",
    "    else:\n",
    "        bias = np.array(0.1)\n",
    "\n",
    "    bias = bias[np.newaxis, np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    if 'scaled_output' in layers:\n",
    "        scale = layers['scaled_output'].scales.get_value()\n",
    "    elif 'scale' in layers:\n",
    "        scale = layers['scale'].scales.get_value()\n",
    "    else:\n",
    "        scale = np.array((1.,))\n",
    "    scale = scale[np.newaxis, np.newaxis, :, np.newaxis, np.newaxis]\n",
    "\n",
    "    \n",
    "    X = T.tensor4()\n",
    "\n",
    "    B = L.get_output(layers['brush'], X)\n",
    "    if len(layers['brush'].output_shape) == 4: # (ex, t, w, h)\n",
    "        B = B.dimshuffle(0, 1, 'x', 2, 3)\n",
    "    \n",
    "    fn = theano.function(\n",
    "        [X], \n",
    "        T.nnet.sigmoid(B * scale + bias)\n",
    "    )\n",
    "    return fn\n",
    "\n",
    "def build_encode_func(layers):\n",
    "    w = layers['output'].output_shape[2]\n",
    "    X = T.tensor4()\n",
    "    fn = theano.function(\n",
    "        [X], \n",
    "        T.nnet.sigmoid(L.get_output(layers['coord'], X)[:, :, 0:2]) * w\n",
    "    )\n",
    "    return fn\n",
    "\n",
    "def to_grid_of_images(seq_imgs, **kw):\n",
    "    y = seq_imgs\n",
    "    imgs = []\n",
    "    for t in range(y.shape[1]):\n",
    "        yy = y[:, t]\n",
    "        if yy.shape[1] == 1:\n",
    "            yy = yy[:, 0, :, :, np.newaxis] * np.ones((1, 1, 1, 3))\n",
    "        else:\n",
    "            yy = yy.transpose((0, 2, 3, 1))\n",
    "        img = dispims_color(yy, **kw)\n",
    "        imgs.append(img)\n",
    "    return imgs\n",
    "\n",
    "def seq_to_video(seq, filename='out.mp4', verbose=1, framerate=8, rate=8, **kw):\n",
    "    # shape of seq should be : (examples, time, c, w, h)\n",
    "    seq = to_grid_of_images(seq, **kw)\n",
    "    seq = [np.zeros_like(seq[0])] + seq\n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    imgs_to_video(seq, out=filename, verbose=verbose, framerate=framerate, rate=rate)\n",
    "\n",
    "def embed_video(filename):\n",
    "    video = open(filename, 'r+b').read()\n",
    "    encoded = base64.b64encode(video)\n",
    "    return HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii')))\n",
    "def disp_grid(imgs, **kw):\n",
    "    # shape of imgs should be : (examples, color, w, h)\n",
    "    out = dispims_color(imgs.transpose((0, 2, 3, 1)) * np.ones((1, 1, 1, 3)), **kw)\n",
    "    return out\n",
    "\n",
    "\n",
    "def prop_uniques(x):\n",
    "    x = x.reshape((x.shape[0], -1))\n",
    "    x = map(hash_array, x)\n",
    "    return len(set(x)) / float(len(x))\n",
    "\n",
    "def hash_array(x):\n",
    "    return hash(tuple(x))\n",
    "\n",
    "def normalize(x, axis=1):\n",
    "    return (x - x.min(axis=axis, keepdims=True)) / (x.max(axis=axis, keepdims=True) - x.min(axis=axis, keepdims=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and build functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some selected models :\n",
    "models = [\n",
    "    \"../training/brush7/model.pkl\", #0 \n",
    "    \"../jobs/results/df2631577eaf38b002c6c0ae6f1937e1/model.pkl\", #1\n",
    "    \"../jobs/results/68fa4141b9f970f6bde95da53b8e54fe/model.pkl\", #2\n",
    "    \"../jobs/results/88b68a0c06bfb0a837cf7c0b77fe7eb9/model.pkl\", #3\n",
    "    \"../jobs/results/9c70a6ee9340c85b398a31f7d16be962/model.pkl\", #4\n",
    "    \"../jobs/results/bc8b5c4156561f4f09685874d52ba20e/model.pkl\", #5\n",
    "    \"../jobs/results/a50c86acf5a2875dda6e1ac5b8d462c1/model.pkl\", #6\n",
    "    \"../training/brush9/model.pkl\", #7\n",
    "    \"../jobs/results/76f2a5320d2d700ee5943dceb2c3e004/model.pkl\", #8\n",
    "    \"../jobs/results/b38bd0e4e6ac281e6eba581cbe9d62d3/model.pkl\", #9\n",
    "    \"../training/brush12/model.pkl\", #10\n",
    "    \"../jobs/results/8af0f0ff67a2c27a4bede27a9867c5cc/model.pkl\", #11\n",
    "    \"../jobs/results/8fbec06424d9cae3887acf14bc01948b/model.pkl\" #12\n",
    "    \"../training/brush16/model.pkl\",#13\n",
    "    \"../jobs/results/cebca877494edddcdb140b5b8b481964/model.pkl\", #14\n",
    "    \"../jobs/results/96b19e8ff6b43a3b0992f111ecafdb2c/model.pkl\", #15\n",
    "]\n",
    "\n",
    "#0  : (mnist:nice,                   ir:emptyfixated,         strokes:bad)\n",
    "#1  : (mnist:nice and curvy,         ir:noisy blobs,          strokes:very_bad and blurry)\n",
    "#2  : (mnist:ok but a bit blurry,    ir:nice,                 strokes:bad and blurry)\n",
    "#3  : (mnist:very nice but probs,    ir:noisy points,         strokes:bad but not blurry)\n",
    "#4  : (mnist:very nice but blurBlo,  ir:emptyfixated,         strokes:bad but not blurry)\n",
    "#5  : (mnist:very nice and squary,   ir:emptyfixated,         strokes:okayish)\n",
    "#6  : (mnist:very nice and squary,   ir:emptyfixated,         strokes:okayish)\n",
    "#7  : (mnist:very nice and squary,   ir:emptyfixated,         strokes:okayish)\n",
    "#8  : (mnist:almostperfect,          ir:digits_fixatedone,    strokes:bad)\n",
    "#9  : (mnist:almostperfect,          ir:digits_fixatedone,    strokes:bad)\n",
    "#10 : (mnist:almostperfect,          ir:emptyfixated,         strokes:okayish)\n",
    "#11 : (mnist:ok but a blurry,        ir:nice  but blobby,     strokes:bad and blurry)\n",
    "#12 : (mnist:nice,                   ir:nice but fixated,     strokes:bad but not blurry)\n",
    "#13 : (omniglot:nice,                ir:strokesfixated,       strokes:ok)\n",
    "#14 : (omniglot:great,               ir:strokesfixated,       mnist:good!)\n",
    "#15 : (aloi:get the shapes notdeta,  ir:needsnoisecool,       omniglot:funny)\n",
    "filename = np.random.choice(models)\n",
    "model, data, layers, w, h, c = load_model(\"../training/fractal/a5/model.pkl\",\n",
    "                                          force_w=16,\n",
    "                                          force_h=16,\n",
    "                                          dataset='cropped_digits',\n",
    "                                          #force_model_params={'w_out': 64, 'h_out': 64},\n",
    "                                          kw_load_data={'include_test': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(json.dumps(model.hypers['model_params'], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode = build_encode_func(layers) # transforms image to sequence of coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "brush = build_brush_func(layers) # transforms an image to sequence of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstruct = model.reconstruct # reconstructs an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get coords from images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encode(model.preprocess(data.X[0:10])).shape # (examples, time, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = data.X[0:11*11]\n",
    "#X = before_last + np.random.normal(0, 1, size=imgs[:, 0].shape).astype(np.float32) #(for colored images)\n",
    "imgs = brush(model.preprocess(X)) # (examples, time, w, h)\n",
    "#imgs = 1 - imgs\n",
    "seq_to_video(imgs, 'seq.mp4')\n",
    "embed_video('seq.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im1 = disp_grid(model.preprocess(data.X[0:100]), border=1, bordercolor=(.3, .3, .3))\n",
    "im2 = disp_grid(reconstruct(model.preprocess(data.X[0:100])), border=1, bordercolor=(.5, 0, 0))\n",
    "im_mix = np.empty((im1.shape[0], im1.shape[1] + im2.shape[1], 3))\n",
    "im_mix[:, 0:im1.shape[1]] = im1\n",
    "im_mix[:, im1.shape[1]:] = im2\n",
    "imsave('im_mix.png', im_mix)\n",
    "Image('im_mix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iterative refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "nb_iter = 10\n",
    "nb_examples = 100\n",
    "thresh = 0.3\n",
    "use_noise = False\n",
    "\n",
    "# PREP\n",
    "if use_noise: noise = np.random.normal(0, 0.5, size=imgs[:, 0].shape).astype(np.float32) #(for colored images)\n",
    "else: noise = 0\n",
    "if thresh == 'moving':\n",
    "    whitepx_ratio = (data.X>0.5).sum() / np.ones_like(data.X).sum()\n",
    "imgs = np.empty((nb_examples, nb_iter + 1, c, w, h)) # 1 = color channel\n",
    "imgs = imgs.astype(np.float32)\n",
    "#imgs[:, 0] = data.X[0:nb_examples].reshape((nb_examples, 1, h, w)) >0.5\n",
    "imgs[:, 0] = np.random.uniform(size=(nb_examples, c, w, h))\n",
    "\n",
    "scores = []\n",
    "diversities = []\n",
    "\n",
    "# ITERATIOn\n",
    "\n",
    "for i in tqdm(range(1, nb_iter + 1)):\n",
    "    \n",
    "    if use_noise:noise = np.random.normal(0, 1, size=imgs[:, 0].shape).astype(np.float32) #(for colored images)\n",
    "    else:noise = 0\n",
    "        \n",
    "    imgs[:, i] = brush(imgs[:, i - 1] + noise)[:,-1]\n",
    "    if c == 1:\n",
    "        if thresh == 'moving':\n",
    "            vals = imgs[:, i].flatten()\n",
    "            vals = vals[np.argsort(vals)]\n",
    "            thresh_ = vals[-int(whitepx_ratio * len(vals)) - 1]\n",
    "        else:\n",
    "            thresh_ = thresh\n",
    "        if thresh_:\n",
    "            imgs[:, i] = imgs[:, i] > thresh_ # binarize\n",
    "    score = np.abs(imgs[:, i - 1] - imgs[:, i]).sum()\n",
    "    scores.append(score)\n",
    "    diversity = prop_uniques(imgs[:, i])\n",
    "    diversities.append(diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "img = disp_grid(imgs[:, -1], border=1, bordercolor=(0.3, 0, 0))\n",
    "imsave('ir.png', img)\n",
    "Image('ir.png', width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_to_video(imgs, 'ir.mp4', border=0, bordercolor=(0, 0, 0))\n",
    "embed_video('ir.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('Score')\n",
    "plt.plot(np.log(np.array(scores)) / np.log(10))\n",
    "plt.show()\n",
    "plt.title('Diversity')\n",
    "plt.plot(diversities)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding a new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dt_test = load_data('strokes', w=w,h=h) # for grayscale\n",
    "#dt_test = load_data('digits', w=w, h=h)\n",
    "dt_test = load_data('omniglot', w=w, h=h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# load from file\n",
    "\n",
    "nb = 100\n",
    "dt = dt_test.X[0:nb]\n",
    "try:\n",
    "    dt = dt.reshape((nb, c, w, h))\n",
    "except Exception:\n",
    "    dt = dt.reshape((nb, 1, w, h))\n",
    "    dt = dt * np.ones((1, 3, 1, 1))\n",
    "    dt = dt.astype(np.float32)\n",
    "print(dt.shape)\n",
    "rec = reconstruct(dt)\n",
    "print(((rec - dt)**2).mean())\n",
    "\n",
    "im1 = disp_grid(model.preprocess(dt[0:nb]), border=1, bordercolor=(.3, .3, .3))\n",
    "im2 = disp_grid(reconstruct(model.preprocess(rec[0:nb])), border=1, bordercolor=(.5, 0, 0))\n",
    "im_mix = np.empty((im1.shape[0], im1.shape[1] + im2.shape[1], 3))\n",
    "im_mix[:, 0:im1.shape[1]] = im1\n",
    "im_mix[:, im1.shape[1]:] = im2\n",
    "imsave('im_mix_new_dataset.png', im_mix)\n",
    "Image('im_mix_new_dataset.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
