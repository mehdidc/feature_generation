- A model without sparsity (to show sparsity is useful)
- Effect of varying mutation parameters
- Effect of Varying initial samples
- Generation from pixels directly with generation from layers with varied parameters
- See the tradeoff between optimization and diversity
-  Forcing diversity of population using clustering (only individuals within a cluster compete with each other).

1) Compare pixel space vs feature space generation with reconstruction error only and show that feature space leads to less noisier samples. Do this starting from dataset images and random images. This way of doing will lead to dominant samples (species) because we don't force diversity.
We can perform several runs of GA with different seeds if we want different samples. 

2) Force diversity using a a fitness with a reconstruction term and diversity term, or other methods we talked about - to be decided. It will lead to noisier samples than ones obtained by running GA with different seeds. Again do that starting from dataset images and random images. 

3) Do the divergence-convergence thing to have diversity only in the beginning then reconstruction only at the end. This reminds me reinforcement learning/bandits with their exploration-exploitation dilemma, one simple it is done in RL is to shrink the lambda (which controls amount of diversity) to 0 through time. An alternative is to to read island stuff in genetic literature. Again do that starting from dataset images and random images. 